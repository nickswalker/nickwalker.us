---
---

@inproceedings{walker2020perceptions,
  author = {Nick Walker and Kevin Weatherwax and Julian Alchin and Leila Takayama and Maya Cakmak},
  title = {Human Perceptions of a Curious Robot that Performs Off-Task Actions},
  booktitle = {ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  location = {Oxford, UK},
  month = {March},
  year = {2020},
  doi  = {10.1145/3319502.3374821},
  wwwtype = {conference},
  wwwpdf  = {https://hcrlab.cs.washington.edu/assets/pdfs/2020/walker2020perceptions.pdf},
  abstract = {
Researchers have proposed models of curiosity as a means to drive robots to learn and adapt to their environments.
While these models balance goal- and exploration-oriented actions in a mathematically principled manor, it is not understood how users perceive a robot that pursues off-task actions. Motivated by a model of curiosity based on intrinsic rewards, we conducted three online video-surveys with a total of 264 participants, evaluating a variety of curious behaviors. Our results indicate that a robot's off-task actions are perceived as expressions of curiosity, but that these actions lead to a negative impact on perceptions of the robot's competence. When the robot explains or acknowledges its deviation from the primary task, this can partially mitigate the negative effects of off-task actions.
  },
}

@article{thomason2020improving,
title={Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog},
author={Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond J. Mooney},
volume={67},
journal={The Journal of Artificial Intelligence Research (JAIR)},
month={February},
year={2020},
wwwtype={journal},
wwwpdf = {http://www.cs.utexas.edu/users/ml/papers/thomason.jair20.pdf}
}

@inproceedings{jiang2019icaps,
  author = {Yuqian Jiang and Nick Walker and Justin Hart and Peter Stone},
  title = {Open-World Reasoning for Service Robots},
  booktitle = {Proceedings of the 29th International Conference on Automated Planning and Scheduling (ICAPS 2019)},
  location = {Berkeley, CA, USA},
  month = {July},
  year = {2019},
  wwwtype = {conference},
  wwwpdf = {http://www.cs.utexas.edu/%7Epstone/Papers/bib2html-links/ICAPS19-Jiang.pdf},
  wwwvideo = {https://youtu.be/TLXGQDTAZvA},
  abstract = {
  A service robot accepting verbal commands from a human operator is likely to
  encounter requests that reference objects not currently represented in its
  knowledge base. In domestic or office settings, the construction of a
  complete knowledge base would be cumbersome and unlikely to succeed in most
  real-world deployments. The world that such a robot operates in is thus
  "open: in the sense that some objects that it must act on in the real world
  are not described in its internal representation. However, when an operator
  gives a command referencing an object that the robot has not yet observed (
  and thus not incorporated into its knowledge base), we can think of the
  object as being hypothetical to the robot. This paper presents a novel
  method for closing the robot's world model for planning purposes by
  introducing hypothetical objects into the robot's knowledge base, reasoning
  about these hypothetical objects, and acting on these hypotheses in the real
  world. We use our implementation of this method on a domestic service robot
  as an illustrative demonstration to explore how it works in practice.
  },
}

@inproceedings{thomason2019icra,
  title={Improving Grounded Natural Language Understanding through Human-Robot Dialog},
  author={Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond J. Mooney},
  booktitle={International Conference on Robotics and Automation (ICRA)},
  year={2019},
  doi={10.1109/ICRA.2019.8794287},
  wwwtype = {conference},
  wwwpdf = {http://www.cs.utexas.edu/users/ml/papers/thomason.icra19.pdf},
  wwwalso = {thomason18robodial,thomason18mrhrc},
  wwwvideo = {https://youtu.be/PbOfteZ_CJc?t=5},
  abstract = {
  Natural language understanding for robotics can require substantial domain- and platform-specific engineering. For example, for mobile robots to pick-and-place objects in an environment to satisfy human commands, we can specify the language humans use to issue such commands, and connect concept words like red can to physical object properties. One way to alleviate this engineering for a new domain is to enable robots in human environments to adapt dynamically—continually learning new language constructions and perceptual concepts. In this work, we present an end-to-end pipeline for translating natural language commands to discrete robot actions, and use clarification dialogs to jointly improve language parsing and concept grounding. We train and evaluate this agent in a virtual setting on Amazon Mechanical Turk, and we transfer the learned agent to a physical robot platform to demonstrate it in the real world.

  }
}

@InProceedings{hart2018iros,
          author = {Justin W. Hart and Rishi Shah and Sean Kirmani and Nick Walker and Kathryn Baldauf and Nathan John and Peter Stone},
           title = {PRISM: Pose Registration for Integrated Semantic Mapping},

        location = {Madrid, Spain},
           month = {October},
            year = {2018},
       booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
        keywords = {semantic mapping},
             doi = {10.1109/IROS.2018.8593681},
         wwwtype = {conference},
          wwwpdf = {http://www.cs.utexas.edu/%7Epstone/Papers/bib2html-links/IROS18-hart.pdf},
        abstract = {Many robotics applications involve navigating to positions specified in terms of their semantic significance. A robot operating in a hotel may need to deliver room service to a named room. In a hospital, it may need to deliver medication to a patient’s room. The Building-Wide Intelligence Project at UT Austin has been developing a fleet of autonomous mobile robots, called BWIBots, which perform tasks in the computer science department. Tasks include guiding a person, delivering a message, or bringing an object to a location such as an office, lecture hall, or classroom. The process of constructing a map that a robot can use for navigation has been simplified by modern SLAM algorithms. The attachment of semantics to map data, however, remains a tedious manual process of labeling locations in otherwise automatically generated maps. This paper introduces a system called PRISM to automate a step in this process by enabling a robot to localize door signs – a semantic markup intended to aid the human occupants of a building – and to annotate these locations in its map}
}

@InProceedings{svetlik2017aaai,
          author = {Maxwell Svetlik and Matteo Leonetti and Jivko Sinapov and Rishi Shah and Nick Walker and Peter Stone},
           title = {Automatic Curriculum Graph Generation for Reinforcement Learning Agents},
       publisher = {Association for the Advancement of Artificial Intelligence},
        location = {San Francisco, CA},
           month = {February},
            year = {2017},
         wwwtype = {conference},
          wwwpdf = {https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14961/14449},
       wwwposter = {https://doi.org/10.5281/zenodo.3244636},
       booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
        keywords = {curriculum learning; reinforcement learning},
             url = {https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14961/14449},
        abstract = {In recent years, research has shown that transfer learning methods can be leveraged to construct curricula that sequence a series of simpler tasks such that performance on a final target task is improved. A major limitation of existing approaches is that such curricula are handcrafted by humans that are typically domain experts. To address this limitation, we introduce a method to generate a curriculum based on task descriptors and a novel metric of transfer potential. Our method automatically generates a curriculum as a directed acyclic graph (as opposed to a linear sequence as done in existing work). Experiments in both discrete and continuous domains show that our method produces curricula that improve the agent's learning performance when compared to the baseline condition of learning on the target task from scratch.}
}


@InProceedings{schroeder2017,
          address = {San Francisco},
           author = {Eric D. Schroeder and Nicholas Walker and Amanda S. Danko},
        booktitle = {Proceedings of SPIE 10051, Neural Imaging and Sensing},
              doi = {10.1117/12.2249416},
           editor = {Luo, Qingming and Ding, Jun},
             isbn = {9781510605435},
             issn = {16057422},
            month = {feb},
            title = {{Wearable ear EEG for brain interfacing}},
             year = {2017},
          wwwtype = {conference},
         abstract = {Brain-computer interfaces (BCIs) measuring electrical activity via electroencephalogram (EEG) have evolved beyond clinical applications to become wireless consumer products. Typically marketed for meditation and neu- rotherapy, these devices are limited in scope and currently too obtrusive to be a ubiquitous wearable. Stemming from recent advancements made in hearing aid technology, wearables have been shrinking to the point that the necessary sensors, circuitry, and batteries can be fit into a small in-ear wearable device. In this work, an ear-EEG device is created with a novel system for artifact removal and signal interpretation. The small, compact, cost-effective, and discreet device is demonstrated against existing consumer electronics in this space for its signal quality, comfort, and usability. A custom mobile application is developed to process raw EEG from each device and display interpreted data to the user. Artifact removal and signal classification is accomplished via a combination of support matrix machines (SMMs) and soft thresholding of relevant statistical properties.}
}

@comment{SYMPOSIUMS AND REFEREED WORKSHOPS}

@inproceedings{walker2019planrob,
         title = {Desiderata for Planning Systems in General-Purpose Service Robots},
        author = {Nick Walker and Yuqian Jiang and Maya Cakmak and Peter Stone},
     booktitle = {Proceedings of 2019 ICAPS Workshop on Planning and Robotics},
         month = {July},
          year = {2019},
       wwwtype = {workshop},
        wwwpdf = {https://icaps19.icaps-conference.org/workshops/PlanRob/PlanRob_2019_submissions/PlanRob_2019_paper_9.pdf},
     wwwslides = {https://doi.org/10.5281/zenodo.3244795},
      abstract = {General-purpose service robots are expected to undertake a broad range of tasks at the request of users. Knowledge representation and planning systems are essential to flexible autonomous robots, but the field lacks a unified perspective on which features are essential for general-purpose service robots. Progress towards planning and reasoning for general-purpose service robots is hindered by differing assumptions about users, the environment, and the overall robot system. In this position paper, we propose desiderata for planning and reasoning systems to promote general-purpose service robots. Each proposed item draws on our experience with research on service robots in the office and home and on the demands of these environments. Our desiderata emphasize support for natural human-interfaces as well as for robust fallback methods when interactions with humans and the environment fail. We highlight relevant work towards these goals.
  }
}

@incollection{walker2019robocup,
         title = {Neural Semantic Parsing with Anonymization for Command Understanding in General-Purpose Service Robots},
        author = {Nick Walker and Yu-Tang Peng and Maya Cakmak},
     booktitle = {RoboCup 2019: Robot Soccer World Cup {XXIII}},
           doi = {10.1007/978-3-030-35699-6_26},
        series = {Lecture Notes in Artificial Intelligence},
        editor = {Jackrit Suthakorn and Mary-Anne Williams and Tim Niemueller and Stephan Chalup},
     publisher = {Springer},
         month = {July},
          year = {2019},
       wwwtype = {symposium},
        wwwpdf = {https://hcrlab.cs.washington.edu/assets/pdfs/2019/walker2019parsing.pdf},
     wwwslides = {http://doi.org/10.5281/zenodo.3253252},
           url = {https://arxiv.org/abs/1907.01115},
      abstract = {Service robots are envisioned to undertake a wide range of tasks at the request of users. Semantic parsing is one way to convert natural language commands given to these robots into executable representations. Methods for creating semantic parsers, however, rely either on large amounts of data or on engineered lexical features and parsing rules, which has limited their application in robotics. To address this challenge, we propose an approach that leverages neural semantic parsing methods in combination with contextual word embeddings to enable the training of a semantic parser with little data and without domain specific parser engineering. Key to our approach is the use of an anonymized target representation which is more easily learned by the parser. In most cases, this simplified representation can trivially be transformed into an executable format, and in others the parse can be completed through further interaction with the user. We evaluate this approach in the context of the RoboCup@Home General Purpose Service Robot task, where we have collected a corpus of paraphrased versions of commands from the standardized command generator. Our results show that neural semantic parsers can predict the logical form of unseen commands with 89% accuracy. We release our data and the details of our models to encourage further development from the RoboCup and service robotics communities.
                 }
}

@inproceedings{jiang2018fss,
         title = {LAAIR: A Layered Architecture for Autonomous Interactive Robots},
        author = {Yuqian Jiang and Nick Walker and Minkyu Kim and Nicolas Brissonneau and Daniel S. Brown and Justin W. Hart and Scott Niekum and Luis Sentis and Peter Stone},
     booktitle = {AAAI Fall Symposium on Reasoning and Learning in Real-World Systems for Long-Term Autonomy},
         month = {October},
          year = {2018},
       wwwtype = {symposium},
        wwwpdf = {http://rbr.cs.umass.edu/lta/papers/FSS-18_paper_55.pdf},
           url = {https://arxiv.org/abs/1811.03563},
      abstract = {When developing general purpose robots, the overarching software architecture can greatly affect the ease of accomplishing various tasks. Initial efforts to create unified robot systems in the 1990s led to hybrid architectures, emphasizing a hierarchy in which deliberative plans direct the use of reactive skills. However, since that time there has been significant progress in the low-level skills available to robots, including manipulation and perception, making it newly feasible to accomplish many more tasks in real-world domains. There is thus renewed optimism that robots will be able to perform a wide array of tasks while maintaining responsiveness to human operators. However, the top layer in traditional hybrid architectures, designed to achieve long-term goals, can make it difficult to react quickly to human interactions during goal-driven execution. To mitigate this difficulty, we propose a novel architecture that supports such transitions by adding a top-level reactive module which has flexible access to both reactive skills and a deliberative control module. To validate this architecture, we present a case study of its application on a domestic service robot platform.
  }
}

@inproceedings{hart2018fss,
         title = {Interaction and Autonomy in RoboCup@Home and Building-Wide Intelligence},
        author = {Justin Hart and Harel Yedidsion and Yuqian Jiang and Nick Walker and Rishi Shah and Jesse Thomason and Aishwarya Padmakumar and Rolando Fernandez and Jivko Sinapov and Raymond Mooney and Peter Stone},
     booktitle = {AAAI Fall Symposium on Artificial Intelligence and Human-Robot Interaction},
         month = {October},
          year = {2018},
       wwwtype = {symposium},
        wwwpdf = {https://arxiv.org/pdf/1810.02919.pdf},
           url = {https://arxiv.org/abs/1810.02919},
      abstract = {Efforts are underway at UT Austin to build autonomous robot systems that address the challenges of long-term deployments in office environments and of the more prescribed domestic service tasks of the RoboCup@Home competition. We discuss the contrasts and synergies of these efforts, highlighting how our work to build a RoboCup@Home Domestic Standard Platform League entry led us to identify an integrated software architecture that could support both projects. Further, naturalistic deployments of our office robot platform as part of the Building-Wide Intelligence project have led us to identify and research new problems in a traditional laboratory setting.}
}

@inproceedings{thomason2018mrhrc,
         title = {Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog},
        author = {Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond J. Mooney},
     booktitle = {Proceedings of the RSS Workshop on Models and Representations for Natural Human-Robot Communication (MRHRC-18)},
     publisher = {Robotics: Science and Systems (RSS)},
         month = {June},
          year = {2018},
       wwwtype = {workshop},
     wwwhidden = {true},
        wwwpdf = {http://www.cs.utexas.edu/%7Epstone/Papers/bib2html-links/MRHRC18-thomason.pdf},
      abstract = {Natural language understanding in robots needs to be robust to a wide-range of both human speakers and human environments. Rather than force humans to use language that robots can understand, robots in human environments should dynamically adapt—continuously learning new language constructions and perceptual concepts as they are used in context. In this work, we present methods for parsing natural language to underlying meanings, and using robotic sensors to create multi-modal models of perceptual concepts. We combine these steps towards language understanding into a holistic agent for jointly improving parsing and perception on a robotic platform through human-robot dialog. We train and evaluate this agent on Amazon Mechanical Turk, then demonstrate it on a robotic platform initialized from conversational data gathered from Mechanical Turk. Our experiments show that improving both parsing and perception components from conversations improves communication quality and human ratings of the agent.}
}

@comment{LIGHTLY REFEREED}
@inproceedings{thomason2018robodial,
         title = {Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog},
        author = {Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond J. Mooney},
     booktitle = {Late-breaking Track at the SIGDIAL Special Session on Physically Situated Dialogue (RoboDIAL-18)},
         month = {July},
          year = {2018},
       wwwtype = {lbr},
     wwwhidden = {true},
      abstract = {In this work, we present methods for parsing natural language to underlying meanings, and using robotic sensors to create multi-modal models of perceptual concepts. We combine these steps towards language understanding into a holistic agent for jointly improving parsing and perception on a robotic platform through human-robot dialog. We train and evaluate this agent on Amazon Mechanical Turk, then demonstrate it on a robotic platform initialized from that conversational data. Our experiments show that improving both parsing and perception components from conversations improves communication quality and human ratings of the agent.}
}

@comment{WORKING}

@article{kim2019,
  author    = {Minkyu Kim and
               Miguel Arduengo and
               Nick Walker and
               Yuqian Jiang and
               Justin W. Hart and
               Peter Stone and
               Luis Sentis},
  title     = {An Architecture for Person-Following using Active Target Search},
  journal   = {CoRR},
  volume    = {abs/1809.08793},
  year      = {2019},
  url       = {http://arxiv.org/abs/1809.08793},
  archivePrefix = {arXiv},
  eprint    = {1809.08793},
  wwwtype = {working},
  abstract = {This paper addresses a novel architecture for person-following robots using active search. The proposed system can be applied in real-time to general mobile robots for learning features of a human, detecting and tracking, and finally navigating towards that person. To succeed at person- following, perception, planning, and robot behavior need to be integrated properly. Toward this end, an active target searching capability, including prediction and navigation toward vantage locations for finding human targets, is proposed. The proposed capability aims at improving the robustness and efficiency for tracking and following people under dynamic conditions such as crowded environments. A multi-modal sensor information approach including fusing an RGB-D sensor and a laser scanner, is pursued to robustly track and identify human targets. Bayesian filtering for keeping track of human and a regression algorithm to predict the trajectory of people are investigated. In order to make the robot autonomous, the proposed framework relies on a behavior-tree structure. Using Toyota Human Support Robot (HSR), real-time experiments demonstrate that the proposed architecture can generate fast, efficient person-following behaviors.}
}