---
---
@InProceedings{jiang2019icaps,
  author = {Yuqian Jiang and Nick Walker and Justin Hart and Peter Stone},
  title = {Open-World Reasoning for Service Robots},
  booktitle = {Proceedings of the 29th International Conference on Automated Planning and Scheduling (ICAPS 2019)},
  location = {Berkeley, CA, USA},
  month = {July},
  year = {2019},
  wwwtype = {conference},
  wwwpdf = {http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICAPS19-Jiang.pdf},
  wwwvideo = {https://youtu.be/TLXGQDTAZvA},
  abstract = {
  A service robot accepting verbal commands from a human operator is likely to
  encounter requests that reference objects not currently represented in its
  knowledge base. In domestic or office settings, the construction of a
  complete knowledge base would be cumbersome and unlikely to succeed in most
  real-world deployments. The world that such a robot operates in is thus
  "open: in the sense that some objects that it must act on in the real world
  are not described in its internal representation. However, when an operator
  gives a command referencing an object that the robot has not yet observed (
  and thus not incorporated into its knowledge base), we can think of the
  object as being hypothetical to the robot. This paper presents a novel
  method for closing the robot's world model for planning purposes by
  introducing hypothetical objects into the robot's knowledge base, reasoning
  about these hypothetical objects, and acting on these hypotheses in the real
  world. We use our implementation of this method on a domestic service robot
  as an illustrative demonstration to explore how it works in practice.
  },
}

@inproceedings{thomason2019icra,
  title={Improving Grounded Natural Language Understanding through Human-Robot Dialog},
  author={Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond J. Mooney},
  booktitle={International Conference on Robotics and Automation (ICRA)},
  year={2019},
  wwwtype = {conference},
  wwwpdf = {https://arxiv.org/pdf/1903.00122.pdf},
  wwwalso = {thomason18robodial,thomason18mrhrc},
  wwwvideo = {https://youtu.be/PbOfteZ_CJc?t=5},
  abstract = {
  Natural language understanding for robotics can require substantial domain- and platform-specific engineering. For example, for mobile robots to pick-and-place objects in an environment to satisfy human commands, we can specify the language humans use to issue such commands, and connect concept words like red can to physical object properties. One way to alleviate this engineering for a new domain is to enable robots in human environments to adapt dynamically—continually learning new language constructions and perceptual concepts. In this work, we present an end-to-end pipeline for translating natural language commands to discrete robot actions, and use clarification dialogs to jointly improve language parsing and concept grounding. We train and evaluate this agent in a virtual setting on Amazon Mechanical Turk, and we transfer the learned agent to a physical robot platform to demonstrate it in the real world.

  }
}

@InProceedings{hart2018iros,
          author = {Justin W. Hart and Rishi Shah and Sean Kirmani and Nick Walker and Kathryn Baldauf and Nathan John and Peter Stone},
           title = {PRISM: Pose Registration for Integrated Semantic Mapping},

        location = {Madrid, Spain},
           month = {October},
            year = {2018},
       booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
        keywords = {semantic mapping},
         wwwtype = {conference},
          wwwpdf = {http://www.cs.utexas.edu/%7Epstone/Papers/bib2html-links/IROS18-hart.pdf},
        abstract = {Many robotics applications involve navigating to positions specified in terms of their semantic significance. A robot operating in a hotel may need to deliver room service to a named room. In a hospital, it may need to deliver medication to a patient’s room. The Building-Wide Intelligence Project at UT Austin has been developing a fleet of autonomous mobile robots, called BWIBots, which perform tasks in the computer science department. Tasks include guiding a person, delivering a message, or bringing an object to a location such as an office, lecture hall, or classroom. The process of constructing a map that a robot can use for navigation has been simplified by modern SLAM algorithms. The attachment of semantics to map data, however, remains a tedious manual process of labeling locations in otherwise automatically generated maps. This paper introduces a system called PRISM to automate a step in this process by enabling a robot to localize door signs – a semantic markup intended to aid the human occupants of a building – and to annotate these locations in its map}
}

@InProceedings{svetlik2017aaai,
          author = {Maxwell Svetlik and Matteo Leonetti and Jivko Sinapov and Rishi Shah and Nick Walker and Peter Stone},
           title = {Automatic Curriculum Graph Generation for Reinforcement Learning Agents},
       publisher = {Association for the Advancement of Artificial Intelligence},
        location = {San Francisco, CA},
           month = {February},
            year = {2017},
         wwwtype = {conference},
          wwwpdf = {https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14961/14449},
       wwwposter = {http://doi.org/10.5281/zenodo.3244636},
       booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
        keywords = {curriculum learning; reinforcement learning},
             url = {https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14961/14449},
        abstract = {In recent years, research has shown that transfer learning methods can be leveraged to construct curricula that sequence a series of simpler tasks such that performance on a final target task is improved. A major limitation of existing approaches is that such curricula are handcrafted by humans that are typically domain experts. To address this limitation, we introduce a method to generate a curriculum based on task descriptors and a novel metric of transfer potential. Our method automatically generates a curriculum as a directed acyclic graph (as opposed to a linear sequence as done in existing work). Experiments in both discrete and continuous domains show that our method produces curricula that improve the agent's learning performance when compared to the baseline condition of learning on the target task from scratch.}
}


@InProceedings{schroeder2017,
          address = {San Francisco},
           author = {Eric D. Schroeder and Nicholas Walker and Amanda S. Danko},
        booktitle = {Proceedings of SPIE 10051, Neural Imaging and Sensing},
              doi = {10.1117/12.2249416},
           editor = {Luo, Qingming and Ding, Jun},
             isbn = {9781510605435},
             issn = {16057422},
            month = {feb},
            title = {{Wearable ear EEG for brain interfacing}},
             year = {2017},
          wwwtype = {conference},
         abstract = {Brain-computer interfaces (BCIs) measuring electrical activity via electroencephalogram (EEG) have evolved beyond clinical applications to become wireless consumer products. Typically marketed for meditation and neu- rotherapy, these devices are limited in scope and currently too obtrusive to be a ubiquitous wearable. Stemming from recent advancements made in hearing aid technology, wearables have been shrinking to the point that the necessary sensors, circuitry, and batteries can be fit into a small in-ear wearable device. In this work, an ear-EEG device is created with a novel system for artifact removal and signal interpretation. The small, compact, cost-effective, and discreet device is demonstrated against existing consumer electronics in this space for its signal quality, comfort, and usability. A custom mobile application is developed to process raw EEG from each device and display interpreted data to the user. Artifact removal and signal classification is accomplished via a combination of support matrix machines (SMMs) and soft thresholding of relevant statistical properties.}
}

@comment{SYMPOSIUMS AND REFEREED WORKSHOPS}

@inproceedings{jiang18fss,
         title = {LAAIR: A Layered Architecture for Autonomous Interactive Robots},
        author = {Yuqian Jiang and Nick Walker and Minkyu Kim and Nicolas Brissonneau and Daniel S. Brown and Justin W. Hart and Scott Niekum and Luis Sentis and Peter Stone},
     booktitle = {AAAI Fall Symposium on Reasoning and Learning in Real-World Systems for Long-Term Autonomy},
         month = {October},
          year = {2018},
       wwwtype = {symposium},
        wwwpdf = {https://arxiv.org/abs/1811.03563.pdf},
           url = {https://arxiv.org/abs/1811.03563},
      abstract = {When developing general purpose robots, the overarching software architecture can greatly affect the ease of accomplishing various tasks. Initial efforts to create unified robot systems in the 1990s led to hybrid architectures, emphasizing a hierarchy in which deliberative plans direct the use of reactive skills. However, since that time there has been significant progress in the low-level skills available to robots, including manipulation and perception, making it newly feasible to accomplish many more tasks in real-world domains. There is thus renewed optimism that robots will be able to perform a wide array of tasks while maintaining responsiveness to human operators. However, the top layer in traditional hybrid architectures, designed to achieve long-term goals, can make it difficult to react quickly to human interactions during goal-driven execution. To mitigate this difficulty, we propose a novel architecture that supports such transitions by adding a top-level reactive module which has flexible access to both reactive skills and a deliberative control module. To validate this architecture, we present a case study of its application on a domestic service robot platform.
  }
}

@inproceedings{hart18fss,
         title = {Interaction and Autonomy in RoboCup@Home and Building-Wide Intelligence},
        author = {Justin Hart and Harel Yedidsion and Yuqian Jiang and Nick Walker and Rishi Shah and Jesse Thomason and Aishwarya Padmakumar and Rolando Fernandez and Jivko Sinapov and Raymond Mooney and Peter Stone},
     booktitle = {AAAI Fall Symposium on Artificial Intelligence and Human-Robot Interaction},
         month = {October},
          year = {2018},
       wwwtype = {symposium},
        wwwpdf = {https://arxiv.org/pdf/1810.02919.pdf},
           url = {https://arxiv.org/abs/1810.02919},
      abstract = {Efforts are underway at UT Austin to build autonomous robot systems that address the challenges of long-term deployments in office environments and of the more prescribed domestic service tasks of the RoboCup@Home competition. We discuss the contrasts and synergies of these efforts, highlighting how our work to build a RoboCup@Home Domestic Standard Platform League entry led us to identify an integrated software architecture that could support both projects. Further, naturalistic deployments of our office robot platform as part of the Building-Wide Intelligence project have led us to identify and research new problems in a traditional laboratory setting.}
}

@inproceedings{thomason18robodial,
         title = {Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog},
        author = {Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond J. Mooney},
     booktitle = {Late-breaking Track at the SIGDIAL Special Session on Physically Situated Dialogue (RoboDIAL-18)},
         month = {July},
          year = {2018},
       wwwtype = {workshop},
     wwwhidden = {true},
      abstract = {In this work, we present methods for parsing natural language to underlying meanings, and using robotic sensors to create multi-modal models of perceptual concepts. We combine these steps towards language understanding into a holistic agent for jointly improving parsing and perception on a robotic platform through human-robot dialog. We train and evaluate this agent on Amazon Mechanical Turk, then demonstrate it on a robotic platform initialized from that conversational data. Our experiments show that improving both parsing and perception components from conversations improves communication quality and human ratings of the agent.}
}

@inproceedings{thomason18mrhrc,
         title = {Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog},
        author = {Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond J. Mooney},
     booktitle = {Proceedings of the RSS Workshop on Models and Representations for Natural Human-Robot Communication (MRHRC-18)},
     publisher = {Robotics: Science and Systems (RSS)},
         month = {June},
          year = {2018},
       wwwtype = {workshop},
     wwwhidden = {true},
        wwwpdf = {http://www.cs.utexas.edu/%7Epstone/Papers/bib2html-links/MRHRC18-thomason.pdf},
      abstract = {Natural language understanding in robots needs to be robust to a wide-range of both human speakers and human environments. Rather than force humans to use language that robots can understand, robots in human environments should dynamically adapt—continuously learning new language constructions and perceptual concepts as they are used in context. In this work, we present methods for parsing natural language to underlying meanings, and using robotic sensors to create multi-modal models of perceptual concepts. We combine these steps towards language understanding into a holistic agent for jointly improving parsing and perception on a robotic platform through human-robot dialog. We train and evaluate this agent on Amazon Mechanical Turk, then demonstrate it on a robotic platform initialized from conversational data gathered from Mechanical Turk. Our experiments show that improving both parsing and perception components from conversations improves communication quality and human ratings of the agent.}
}

@comment{WORKING}

@article{kim2018,
  author    = {Minkyu Kim and
               Miguel Arduengo and
               Nick Walker and
               Yuqian Jiang and
               Justin W. Hart and
               Peter Stone and
               Luis Sentis},
  title     = {An Architecture for Person-Following using Active Target Search},
  journal   = {CoRR},
  volume    = {abs/1809.08793},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.08793},
  archivePrefix = {arXiv},
  eprint    = {1809.08793},
  wwwtype = {working},
}